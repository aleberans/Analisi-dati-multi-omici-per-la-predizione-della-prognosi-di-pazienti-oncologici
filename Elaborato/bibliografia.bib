@Inbook{Qi2012,
	author="Qi, Yanjun",
	editor="Zhang, Cha
	and Ma, Yunqian",
	title="Random Forest for Bioinformatics",
	bookTitle="Ensemble Machine Learning: Methods and Applications",
	year="2012",
	publisher="Springer US",
	address="Boston, MA",
	pages="307--323",
	abstract="Modern biology has experienced an increased use of machine learning techniques for large scale and complex biological data analysis. In the area of Bioinformatics, the Random Forest (RF) [6] technique, which includes an ensemble of decision trees and incorporates feature selection and interactions naturally in the learning process, is a popular choice. It is nonparametric, interpretable, efficient, and has high prediction accuracy for many types of data. Recent work in computational biology has seen an increased use of RF, owing to its unique advantages in dealing with small sample size, high-dimensional feature space, and complex data structures.",
	isbn="978-1-4419-9326-7",
	doi="10.1007/978-1-4419-9326-7\_11",
	url="https://doi.org/10.1007/978-1-4419-9326-7\_11"
}

@Inbook{Cunningham2008,
	author="Cunningham, P{\'a}draig
	and Cord, Matthieu
	and Delany, Sarah Jane",
	editor="Cord, Matthieu
	and Cunningham, P{\'a}draig",
	title="Supervised Learning",
	bookTitle="Machine Learning Techniques for Multimedia: Case Studies on Organization and Retrieval",
	year="2008",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="21--49",
	abstract="Supervised learning accounts for a lot of research activity in machine learning and many supervised learning techniques have found application in the processing of multimedia content. The defining characteristic of supervised learning is the availability of annotated training data. The name invokes the idea of a `supervisor' that instructs the learning system on the labels to associate with training examples. Typically these labels are class labels in classification problems. Supervised learning algorithms induce models from these training data and these models can be used to classify other unlabelled data. In this chapter we ground or analysis of supervised learning on the theory of risk minimization. We provide an overview of support vector machines and nearest neighbour classifiers{\textasciitilde}-- probably the two most popular supervised learning techniques employed in multimedia research.",
	isbn="978-3-540-75171-7",
	doi="10.1007/978-3-540-75171-7_2",
	url="https://doi.org/10.1007/978-3-540-75171-7_2"
}

@Article{Kingsford2008,
	author={Kingsford, Carl
	and Salzberg, Steven L.},
	title={What are decision trees?},
	journal={Nature Biotechnology},
	year={2008},
	month={Sep},
	day={01},
	volume={26},
	number={9},
	pages={1011-1013},
	abstract={Decision trees have been applied to problems such as assigning protein function and predicting splice sites. How do these classifiers work, what types of problems can they solve and what are their advantages over alternatives?},
	issn={1546-1696},
}

@article{Machine_Learning_in_Bioinformatics,
	author = {Larrañaga, Pedro and Calvo, Borja and Santana, Roberto and Bielza, Concha and Galdiano, Josu and Inza, Iñaki and Lozano, José A. and Armañanzas, Rubén and Santafé, Guzmán and Pérez, Aritz and Robles, Victor},
	title = "{Machine learning in bioinformatics}",
	journal = {Briefings in Bioinformatics},
	volume = {7},
	number = {1},
	pages = {86-112},
	year = {2006},
	month = {03},
	abstract = "{This article reviews machine learning methods for bioinformatics. It presents modelling methods, such as supervised classification, clustering and probabilistic graphical models for knowledge discovery, as well as deterministic and stochastic heuristics for optimization. Applications in genomics, proteomics, systems biology, evolution and text mining are also shown.}",
	issn = {1467-5463},
	doi = {10.1093/bib/bbk007},
	url = {https://doi.org/10.1093/bib/bbk007},
	eprint = {https://academic.oup.com/bib/article-pdf/7/1/86/23992771/bbk007.pdf},
}

@article{MIAO2016919,
	title = {A Survey on Feature Selection},
	journal = {Procedia Computer Science},
	volume = {91},
	pages = {919-926},
	year = {2016},
	note = {Promoting Business Analytics and Quantitative Management of Technology: 4th International Conference on Information Technology and Quantitative Management (ITQM 2016)},
	issn = {1877-0509},
	doi = {https://doi.org/10.1016/j.procs.2016.07.111},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050916313047},
	author = {Jianyu Miao and Lingfeng Niu},
	keywords = {feature selection, machine learning, unsupervised, clustering},
	abstract = {Feature selection, as a dimensionality reduction technique, aims to choosing a small subset of the relevant features from the original features by removing irrelevant, redundant or noisy features. Feature selection usually can lead to better learning performance, i.e., higher learning accuracy, lower computational cost, and better model interpretability. Recently, researchers from computer vision, text mining and so on have proposed a variety of feature selection algorithms and in terms of theory and experiment, show the effectiveness of their works. This paper is aimed at reviewing the state of the art on these techniques. Furthermore, a thorough experiment is conducted to check if the use of feature selection can improve the performance of learning, considering some of the approaches mentioned in the literature. The experimental results show that unsupervised feature selection algorithms benefits machine learning tasks improving the performance of clustering.}
}

@book{smlbook,
	author = {Lindholm, Andreas and Wahlstr\"om, Niklas and Lindsten, Fredrik and Sch\"on, Thomas B.},
	year = 2022,
	title = {Machine Learning - A First Course for Engineers and Scientists},
	publisher = {Cambridge University Press},
	URL={https://smlbook.org},
}

@ARTICLE{Casiraghi2020-zt,
	title    = "Explainable Machine Learning for Early Assessment of {COVID-19}
	Risk Prediction in Emergency Departments",
	author   = "Casiraghi, Elena and Malchiodi, Dario and Trucco, Gabriella and
	Frasca, Marco and Cappelletti, Luca and Fontana, Tommaso and
	Esposito, Alessandro Andrea and Avola, Emanuele and Jachetti,
	Alessandro and Reese, Justin and Rizzi, Alessandro and Robinson,
	Peter N and Valentini, Giorgio",
	abstract = "Between January and October of 2020, the severe acute respiratory
	syndrome coronavirus 2 (SARS-CoV-2) virus has infected more than
	34 million persons in a worldwide pandemic leading to over one
	million deaths worldwide (data from the Johns Hopkins
	University). Since the virus begun to spread, emergency
	departments were busy with COVID-19 patients for whom a quick
	decision regarding in- or outpatient care was required. The virus
	can cause characteristic abnormalities in chest radiographs
	(CXR), but, due to the low sensitivity of CXR, additional
	variables and criteria are needed to accurately predict risk.
	Here, we describe a computerized system primarily aimed at
	extracting the most relevant radiological, clinical, and
	laboratory variables for improving patient risk prediction, and
	secondarily at presenting an explainable machine learning system,
	which may provide simple decision criteria to be used by
	clinicians as a support for assessing patient risk. To achieve
	robust and reliable variable selection, Boruta and Random Forest
	(RF) are combined in a 10-fold cross-validation scheme to produce
	a variable importance estimate not biased by the presence of
	surrogates. The most important variables are then selected to
	train a RF classifier, whose rules may be extracted, simplified,
	and pruned to finally build an associative tree, particularly
	appealing for its simplicity. Results show that the radiological
	score automatically computed through a neural network is highly
	correlated with the score computed by radiologists, and that
	laboratory variables, together with the number of comorbidities,
	aid risk prediction. The prediction performance of our approach
	was compared to that that of generalized linear models and shown
	to be effective and robust. The proposed machine learning-based
	computational system can be easily deployed and used in emergency
	departments for rapid and accurate risk prediction in COVID-19
	patients.",
	journal  = "IEEE Access",
	volume   =  8,
	pages    = "196299--196325",
	month    =  oct,
	year     =  2020,
	address  = "United States",
	keywords = "Associative tree; Boruta feature selection; COVID-19; clinical
	data analysis; generalized linear models; missing data
	imputation; random forest classifier; risk prediction",
	language = "en"
}

@misc{https://doi.org/10.48550/arxiv.2112.12705,
	doi = {10.48550/ARXIV.2112.12705},
	
	url = {https://arxiv.org/abs/2112.12705},
	
	author = {Giuste, Felipe and Shi, Wenqi and Zhu, Yuanda and Naren, Tarun and Isgut, Monica and Sha, Ying and Tong, Li and Gupte, Mitali and Wang, May D.},
	
	keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Explainable Artificial Intelligence Methods in Combating Pandemics: A Systematic Review},
	
	publisher = {arXiv},
	
	year = {2021},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{Schubach2017,
	author={Schubach, Max
	and Re, Matteo
	and Robinson, Peter N.
	and Valentini, Giorgio},
	title={Imbalance-Aware Machine Learning for Predicting Rare and Common Disease-Associated Non-Coding Variants},
	journal={Scientific Reports},
	year={2017},
	month={Jun},
	day={07},
	volume={7},
	number={1},
	pages={2959},
	abstract={Disease and trait-associated variants represent a tiny minority of all known genetic variation, and therefore there is necessarily an imbalance between the small set of available disease-associated and the much larger set of non-deleterious genomic variation, especially in non-coding regulatory regions of human genome. Machine Learning (ML) methods for predicting disease-associated non-coding variants are faced with a chicken and egg problem - such variants cannot be easily found without ML, but ML cannot begin to be effective until a sufficient number of instances have been found. Most of state-of-the-art ML-based methods do not adopt specific imbalance-aware learning techniques to deal with imbalanced data that naturally arise in several genome-wide variant scoring problems, thus resulting in a significant reduction of sensitivity and precision. We present a novel method that adopts imbalance-aware learning strategies based on resampling techniques and a hyper-ensemble approach that outperforms state-of-the-art methods in two different contexts: the prediction of non-coding variants associated with Mendelian and with complex diseases. We show that imbalance-aware ML is a key issue for the design of robust and accurate prediction algorithms and we provide a method and an easy-to-use software tool that can be effectively applied to this challenging prediction task.},
	issn={2045-2322},
	doi={10.1038/s41598-017-03011-5},
	url={https://doi.org/10.1038/s41598-017-03011-5}
}

@Article{Sarker2021,
	author={Sarker, Iqbal H.},
	title={Machine Learning: Algorithms, Real-World Applications and Research Directions},
	journal={SN Computer Science},
	year={2021},
	month={Mar},
	day={22},
	volume={2},
	number={3},
	pages={160},
	abstract={In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study's key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers in various real-world situations and application areas, particularly from the technical point of view.},
	issn={2661-8907},
	doi={10.1007/s42979-021-00592-x},
	url={https://doi.org/10.1007/s42979-021-00592-x}
}

@article{10.1093/bioinformatics/15.6.510,
	author = {Baker, P G and Goble, C A and Bechhofer, S and Paton, N W and Stevens, R and Brass, A},
	title = "{An ontology for bioinformatics applications.}",
	journal = {Bioinformatics},
	volume = {15},
	number = {6},
	pages = {510-520},
	year = {1999},
	month = {06},
	abstract = "{MOTIVATION: An ontology of biological terminology provides a model of biological concepts that can be used to form a semantic framework for many data storage, retrieval and analysis tasks. Such a semantic framework could be used to underpin a range of important bioinformatics tasks, such as the querying of heterogeneous bioinformatics sources or the systematic annotation of experimental results. RESULTS: This paper provides an overview of an ontology [the Transparent Access to Multiple Biological Information Sources (TAMBIS) ontology or TaO] that describes a wide range of bioinformatics concepts. The present paper describes the mechanisms used for delivering the ontology and discusses the ontology's design and organization, which are crucial for maintaining the coherence of a large collection of concepts and their relationships. AVAILABILITY: The TAMBIS system, which uses a subset of the TaO described here, is accessible over the Web via http://img.cs.man.ac.uk/tambis (although in the first instance, we will use a password mechanism to limit the load on our server). The complete model is also available on the Web at the above URL.}",
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/15.6.510},
	url = {https://doi.org/10.1093/bioinformatics/15.6.510},
	eprint = {https://academic.oup.com/bioinformatics/article-pdf/15/6/510/9732048/150510.pdf},
}

@ARTICLE{Mathe2002-us,
	title    = "Current methods of gene prediction, their strengths and
	weaknesses",
	author   = "Math{\'e}, Catherine and Sagot, Marie-France and Schiex, Thomas
	and Rouz{\'e}, Pierre",
	abstract = "While the genomes of many organisms have been sequenced over the
	last few years, transforming such raw sequence data into
	knowledge remains a hard task. A great number of prediction
	programs have been developed that try to address one part of this
	problem, which consists of locating the genes along a genome.
	This paper reviews the existing approaches to predicting genes in
	eukaryotic genomes and underlines their intrinsic advantages and
	limitations. The main mathematical models and computational
	algorithms adopted are also briefly described and the resulting
	software classified according to both the method and the type of
	evidence used. Finally, the several difficulties and pitfalls
	encountered by the programs are detailed, showing that
	improvements are needed and that new directions must be
	considered.",
	journal  = "Nucleic Acids Res",
	volume   =  30,
	number   =  19,
	pages    = "4103--4117",
	month    =  oct,
	year     =  2002,
	address  = "England",
	language = "en"
}

@ARTICLE{Aerts2004-jh,
	title    = "A genetic algorithm for the detection of new cis-regulatory
	modules in sets of coregulated genes",
	author   = "Aerts, Stein and Van Loo, Peter and Moreau, Yves and De Moor,
	Bart",
	abstract = "SUMMARY: The implementation of a genetic algorithm is described
	that provides a fast method of searching for the optimal
	combination of transcription factor binding sites in a set of
	regulatory sequences. AVAILABILITY: The algorithm can be used
	transparently as a web service from within the Toucan software.
	Toucan can be accessed at
	http://www.esat.kuleuven.ac.be/~saerts/software/toucan.php. A
	standalone version of the software is available upon request.",
	journal  = "Bioinformatics",
	volume   =  20,
	number   =  12,
	pages    = "1974--1976",
	month    =  mar,
	year     =  2004,
	address  = "England",
	language = "en"
}


@ARTICLE{Carter2001-oo,
	title    = "A computational approach to identify genes for functional {RNAs}
	in genomic sequences",
	author   = "Carter, R J and Dubchak, I and Holbrook, S R",
	abstract = "Currently there is no successful computational approach for
	identification of genes encoding novel functional RNAs (fRNAs) in
	genomic sequences. We have developed a machine learning approach
	using neural networks and support vector machines to extract
	common features among known RNAs for prediction of new RNA genes
	in the unannotated regions of prokaryotic and archaeal genomes.
	The Escherichia coli genome was used for development, but we have
	applied this method to several other bacterial and archaeal
	genomes. Networks based on nucleotide composition were 80-90\%
	accurate in jackknife testing experiments for bacteria and
	90-99\% for hyperthermophilic archaea. We also achieved a
	significant improvement in accuracy by combining these
	predictions with those obtained using a second set of parameters
	consisting of known RNA sequence motifs and the calculated free
	energy of folding. Several known fRNAs not included in the
	training datasets were identified as well as several hundred
	predicted novel RNAs. These studies indicate that there are many
	unidentified RNAs in simple genomes that can be predicted
	computationally as a precursor to experimental study. Public
	access to our RNA gene predictions and an interface for user
	predictions is available via the web.",
	journal  = "Nucleic Acids Res",
	volume   =  29,
	number   =  19,
	pages    = "3928--3938",
	month    =  oct,
	year     =  2001,
	address  = "England",
	language = "en"
}

@Article{Cortes1995,
	author={Cortes, Corinna
	and Vapnik, Vladimir},
	title={Support-vector networks},
	journal={Machine Learning},
	year={1995},
	month={Sep},
	day={01},
	volume={20},
	number={3},
	pages={273-297},
	abstract={Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	issn={1573-0565},
	doi={10.1007/BF00994018},
	url={https://doi.org/10.1007/BF00994018}
}

@book{bishop1995neural,
	added-at = {2011-01-24T09:52:55.000+0100},
	author = {Bishop, C.M.},
	biburl = {https://www.bibsonomy.org/bibtex/2992504b34d1e4480654d4262945cb682/a.guidali},
	date-added = {2010-05-25 17:08:37 +0200},
	date-modified = {2010-05-25 17:08:37 +0200},
	interhash = {e7e5ff5e89d29b7fa2787aad12bea123},
	intrahash = {992504b34d1e4480654d4262945cb682},
	keywords = {imported},
	publisher = {Oxford University Press, USA},
	timestamp = {2011-01-24T09:52:56.000+0100},
	title = {{Neural networks for pattern recognition}},
	year = 1995
}


@article{JSSv045i03,
	title={mice: Multivariate Imputation by Chained Equations in R},
	volume={45},
	url={https://www.jstatsoft.org/index.php/jss/article/view/v045i03},
	doi={10.18637/jss.v045.i03},
	abstract={The R package &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt;, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
	number={3},
	journal={Journal of Statistical Software},
	author={van Buuren, Stef and Groothuis-Oudshoorn, Karin},
	year={2011},
	pages={1–67}
}

@article{10.1093/bioinformatics/btr597,
	author = {Stekhoven, Daniel J. and Bühlmann, Peter},
	title = "{MissForest—non-parametric missing value imputation for mixed-type data}",
	journal = {Bioinformatics},
	volume = {28},
	number = {1},
	pages = {112-118},
	year = {2011},
	month = {10},
	abstract = "{Motivation: Modern data acquisition based on high-throughput technology is often facing the problem of missing data. Algorithms commonly used in the analysis of such large-scale data often depend on a complete set. Missing value imputation offers a solution to this problem. However, the majority of available imputation methods are restricted to one type of variable only: continuous or categorical. For mixed-type data, the different types are usually handled separately. Therefore, these methods ignore possible relations between variable types. We propose a non-parametric method which can cope with different types of variables simultaneously.Results: We compare several state of the art methods for the imputation of missing values. We propose and evaluate an iterative imputation method (missForest) based on a random forest. By averaging over many unpruned classification or regression trees, random forest intrinsically constitutes a multiple imputation scheme. Using the built-in out-of-bag error estimates of random forest, we are able to estimate the imputation error without the need of a test set. Evaluation is performed on multiple datasets coming from a diverse selection of biological fields with artificially introduced missing values ranging from 10\\% to 30\\%. We show that missForest can successfully handle missing values, particularly in datasets including different types of variables. In our comparative study, missForest outperforms other methods of imputation especially in data settings where complex interactions and non-linear relations are suspected. The out-of-bag imputation error estimates of missForest prove to be adequate in all settings. Additionally, missForest exhibits attractive computational efficiency and can cope with high-dimensional data.Availability: The ℝ package missForest is freely available from http://stat.ethz.ch/CRAN/.Contact:stekhoven@stat.math.ethz.ch; buhlmann@stat.math.ethz.ch}",
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/btr597},
	url = {https://doi.org/10.1093/bioinformatics/btr597},
	eprint = {https://academic.oup.com/bioinformatics/article-pdf/28/1/112/583703/btr597.pdf},
}


@article{JSSv036i11,
	title={Feature Selection with the Boruta Package},
	volume={36},
	url={https://www.jstatsoft.org/index.php/jss/article/view/v036i11},
	doi={10.18637/jss.v036.i11},
	abstract={This article describes a &amp;lt;b&amp;gt;R&amp;lt;/b&amp;gt; package &amp;lt;b&amp;gt;Boruta&amp;lt;/b&amp;gt;, implementing a novel feature selection algorithm for finding emph{all relevant variables}. The algorithm is designed as a wrapper around a Random Forest classification algorithm. It iteratively removes the features which are proved by a statistical test to be less relevant than random probes. The &amp;lt;b&amp;gt;Boruta&amp;lt;/b&amp;gt; package provides a convenient interface to the algorithm. The short description of the algorithm and examples of its application are presented.},
	number={11},
	journal={Journal of Statistical Software},
	author={Kursa, Miron B. and Rudnicki, Witold R.},
	year={2010},
	pages={1–13}
}

@Article{Kursa2010,
	author={Kursa, Miron B.
	and Jankowski, Aleksander
	and Rudnicki, Witold R.},
	title={Boruta -- A System for Feature Selection},
	journal={Fundamenta Informaticae},
	year={2010},
	publisher={IOS Press},
	volume={101},
	pages={271-285},
	abstract={Machine learning methods are often used to classify objects described by hundreds of attributes; in many applications of this kind a great fraction of attributes may be totally irrelevant to the classification problem. Even more, usually one cannot decide a priori which attributes are relevant. In this paper we present an improved version of the algorithm for identification of the full set of truly important variables in an information system. It is an extension of the random forest method which utilises the importance measure generated by the original algorithm. It compares, in the iterative fashion, the importances of original attributes with importances of their randomised copies. We analyse performance of the algorithm on several examples of synthetic data, as well as on a biologically important problem, namely on identification of the sequence motifs that are important for aptameric activity of short RNA sequences.},
	note={4},
	doi={10.3233/FI-2010-288},
	url={https://doi.org/10.3233/FI-2010-288}
}

@ARTICLE{Altmann2010-sw,
	title    = "Permutation importance: a corrected feature importance measure",
	author   = "Altmann, Andr{\'e} and Tolo{\c s}i, Laura and Sander, Oliver and
	Lengauer, Thomas",
	abstract = "MOTIVATION: In life sciences, interpretability of machine
	learning models is as important as their prediction accuracy.
	Linear models are probably the most frequently used methods for
	assessing feature relevance, despite their relative
	inflexibility. However, in the past years effective estimators of
	feature relevance have been derived for highly complex or
	non-parametric models such as support vector machines and
	RandomForest (RF) models. Recently, it has been observed that RF
	models are biased in such a way that categorical variables with a
	large number of categories are preferred. RESULTS: In this work,
	we introduce a heuristic for normalizing feature importance
	measures that can correct the feature importance bias. The method
	is based on repeated permutations of the outcome vector for
	estimating the distribution of measured importance for each
	variable in a non-informative setting. The P-value of the
	observed importance provides a corrected measure of feature
	importance. We apply our method to simulated data and demonstrate
	that (i) non-informative predictors do not receive significant
	P-values, (ii) informative variables can successfully be
	recovered among non-informative variables and (iii) P-values
	computed with permutation importance (PIMP) are very helpful for
	deciding the significance of variables, and therefore improve
	model interpretability. Furthermore, PIMP was used to correct
	RF-based importance measures for two real-world case studies. We
	propose an improved RF model that uses the significant variables
	with respect to the PIMP measure and show that its prediction
	accuracy is superior to that of other existing models.
	AVAILABILITY: R code for the method presented in this article is
	available at http://www.mpi-inf.mpg.de/ approximately
	altmann/download/PIMP.R CONTACT: altmann@mpi-inf.mpg.de,
	laura.tolosi@mpi-inf.mpg.de SUPPLEMENTARY INFORMATION:
	Supplementary data are available at Bioinformatics online.",
	journal  = "Bioinformatics",
	volume   =  26,
	number   =  10,
	pages    = "1340--1347",
	month    =  apr,
	year     =  2010,
	address  = "England",
	language = "en"
}

@Article{Strobl2007,
	author={Strobl, Carolin
	and Boulesteix, Anne-Laure
	and Zeileis, Achim
	and Hothorn, Torsten},
	title={Bias in random forest variable importance measures: Illustrations, sources and a solution},
	journal={BMC Bioinformatics},
	year={2007},
	month={Jan},
	day={25},
	volume={8},
	number={1},
	pages={25},
	abstract={Variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields, for instance to select a subset of genetic markers relevant for the prediction of a certain disease. We show that random forest variable importance measures are a sensible means for variable selection in many applications, but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories. This is particularly important in genomics and computational biology, where predictors often include variables of different types, for example when predictors include both sequence data and continuous variables such as folding energy, or when amino acid sequence data show different numbers of categories.},
	issn={1471-2105},
	doi={10.1186/1471-2105-8-25},
	url={https://doi.org/10.1186/1471-2105-8-25}
}


@article{e1bb70540db14ceeacddd4383fca9bff,
	title = "CBC: An associative classifier with a small number of rules",
	abstract = "Associative classifiers have been proposed to achieve an accurate model with each individual rule being interpretable. However, existing associative classifiers often consist of a large number of rules and, thus, can be difficult to interpret. We show that associative classifiers consisting of an ordered rule set can be represented as a tree model. From this view, it is clear that these classifiers are restricted in that at least one child node of a non-leaf node is never split. We propose a new tree model, i.e., condition-based tree (CBT), to relax the restriction. Furthermore, we also propose an algorithm to transform a CBT to an ordered rule set with concise rule conditions. This ordered rule set is referred to as a condition-based classifier (CBC). Thus, the interpretability of an associative classifier is maintained, but more expressive models are possible. The rule transformation algorithm can be also applied to regular binary decision trees to extract an ordered set of rules with simple rule conditions. Feature selection is applied to a binary representation of conditions to simplify/improve the models further. Experimental studies show that CBC has competitive accuracy performance, and has a significantly smaller number of rules (median of 10 rules per data set) than well-known associative classifiers such as CBA (median of 47) and GARC (median of 21). CBC with feature selection has even a smaller number of rules.",
	keywords = "Association rule, Decision tree, Feature selection, Rule pruning, Rule-based classifier",
	author = "Houtao Deng and George Runger and Eugene Tuv and Wade Bannister",
	note = "Funding Information: This research was partially supported by ONR grant N00014-09-1-0656 . Copyright: Copyright 2014 Elsevier B.V., All rights reserved.",
	year = "2014",
	month = mar,
	doi = "10.1016/j.dss.2013.11.004",
	language = "English (US)",
	volume = "59",
	pages = "163--170",
	journal = "Decision Support Systems",
	issn = "0167-9236",
	publisher = "Elsevier",
	number = "1",
	
}

@article{10.2307/2344614,
	ISSN = {00359238},
	URL = {http://www.jstor.org/stable/2344614},
	abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
	author = {J. A. Nelder and R. W. M. Wedderburn},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	number = {3},
	pages = {370--384},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Generalized Linear Models},
	urldate = {2023-01-20},
	volume = {135},
	year = {1972}
}

@book{lewis2015applied,
	title={Applied regression: An introduction},
	author={Lewis-Beck, Colin and Lewis-Beck, Michael},
	volume={22},
	year={2015},
	publisher={Sage publications}
}

@article{LearningfromLabeledandUnlabeledDatawithLabelPropagation,
	author = {Zhu, Xiaojin and Ghahramani, Zoubin},
	year = {2003},
	month = {07},
	pages = {},
	title = {Learning from Labeled and Unlabeled Data with Label Propagation}
}

@inproceedings{10.1145/279943.279962,
	author = {Blum, Avrim and Mitchell, Tom},
	title = {Combining Labeled and Unlabeled Data with Co-Training},
	year = {1998},
	isbn = {1581130570},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/279943.279962},
	doi = {10.1145/279943.279962},
	booktitle = {Proceedings of the Eleventh Annual Conference on Computational Learning Theory},
	pages = {92–100},
	numpages = {9},
	location = {Madison, Wisconsin, USA},
	series = {COLT' 98}
}

@inproceedings{inproceedings,
	author = {Wang, Qingyong and Xia, Liang-Yong and Chai, Hua and Zhou, Yun},
	year = {2018},
	month = {10},
	pages = {796-803},
	title = {Semi-Supervised Learning with Ensemble Self-Training for Cancer Classification},
	doi = {10.1109/SmartWorld.2018.00149}
}

@book{busoniu2010reinforcement,
	author = "Busoniu, L. and Babuska, R. and De Schutter and B. and Ernst, D.",
	title = "Reinforcement {Learning} and {Dynamic} {Programming} {Using} {Function} {Approximators} (1st ed.)",
	publisher = "CRC Press",
	year = 2010
}

@misc{TCGASource,
	howpublished = "\url{https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga}"
}

@article{LIU2018400,
	title = {An Integrated TCGA Pan-Cancer Clinical Data Resource to Drive High-Quality Survival Outcome Analytics},
	journal = {Cell},
	volume = {173},
	number = {2},
	pages = {400-416.e11},
	year = {2018},
	issn = {0092-8674},
	doi = {https://doi.org/10.1016/j.cell.2018.02.052},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867418302290},
	author = {Jianfang Liu et al.},
	keywords = {The Cancer Genome Atlas, TCGA, clinical data resource, translational research, follow-up time, overall survival, disease-specific survival, disease-free interval, progression-free interval, Cox proportional hazards regression model},
	abstract = {Summary
	For a decade, The Cancer Genome Atlas (TCGA) program collected clinicopathologic annotation data along with multi-platform molecular profiles of more than 11,000 human tumors across 33 different cancer types. TCGA clinical data contain key features representing the democratized nature of the data collection process. To ensure proper use of this large clinical dataset associated with genomic features, we developed a standardized dataset named the TCGA Pan-Cancer Clinical Data Resource (TCGA-CDR), which includes four major clinical outcome endpoints. In addition to detailing major challenges and statistical limitations encountered during the effort of integrating the acquired clinical data, we present a summary that includes endpoint usage recommendations for each cancer type. These TCGA-CDR findings appear to be consistent with cancer genomics studies independent of the TCGA effort and provide opportunities for investigating cancer biology using clinical correlates at an unprecedented scale.}
}

@article{doi:10.1177/1177932219899051,
	author = {Indhupriya Subramanian and Srikant Verma and Shiva Kumar and Abhay Jere and Krishanpal Anamika},
	title ={Multi-omics Data Integration, Interpretation, and Its Application},
	journal = {Bioinformatics and Biology Insights},
	volume = {14},
	number = {},
	pages = {1177932219899051},
	year = {2020},
	doi = {10.1177/1177932219899051},
	note ={PMID: 32076369},
	
	URL = { 
	https://doi.org/10.1177/1177932219899051
	
	},
	eprint = { 
	https://doi.org/10.1177/1177932219899051
	
	}
	,
	abstract = { To study complex biological processes holistically, it is imperative to take an integrative approach that combines multi-omics data to highlight the interrelationships of the involved biomolecules and their functions. With the advent of high-throughput techniques and availability of multi-omics data generated from a large set of samples, several promising tools and methods have been developed for data integration and interpretation. In this review, we collected the tools and methods that adopt integrative approach to analyze multiple omics data and summarized their ability to address applications such as disease subtyping, biomarker prediction, and deriving insights into the data. We provide the methodology, use-cases, and limitations of these tools; brief account of multi-omics data repositories and visualization portals; and challenges associated with multi-omics data integration. }
}

@Article{Hasin2017,
	author={Hasin, Yehudit
	and Seldin, Marcus
	and Lusis, Aldons},
	title={Multi-omics approaches to disease},
	journal={Genome Biology},
	year={2017},
	month={May},
	day={05},
	volume={18},
	number={1},
	pages={83},
	abstract={High-throughput technologies have revolutionized medical research. The advent of genotyping arrays enabled large-scale genome-wide association studies and methods for examining global transcript levels, which gave rise to the field of ``integrative genetics''. Other omics technologies, such as proteomics and metabolomics, are now often incorporated into the everyday methodology of biological researchers. In this review, we provide an overview of such omics technologies and focus on methods for their integration across multiple omics layers. As compared to studies of a single omics type, multi-omics offers the opportunity to understand the flow of information that underlies disease.},
	issn={1474-760X},
	doi={10.1186/s13059-017-1215-1},
	url={https://doi.org/10.1186/s13059-017-1215-1}
}

@article{MOMENI2020103466,
	title = {A survey on single and multi omics data mining methods in cancer data classification},
	journal = {Journal of Biomedical Informatics},
	volume = {107},
	pages = {103466},
	year = {2020},
	issn = {1532-0464},
	doi = {https://doi.org/10.1016/j.jbi.2020.103466},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046420300939},
	author = {Zahra Momeni and Esmail Hassanzadeh and Mohammad {Saniee Abadeh} and Riccardo Bellazzi},
	keywords = {Cancer classification, Single and multi omics data, Gene selection, High dimensional datasets, Data integration},
	abstract = {Data analytics is routinely used to support biomedical research in all areas, with particular focus on the most relevant clinical conditions, such as cancer. Bioinformatics approaches, in particular, have been used to characterize the molecular aspects of diseases. In recent years, numerous studies have been performed on cancer based upon single and multi-omics data. For example, Single-omics-based studies have employed a diverse set of data, such as gene expression, DNA methylation, or miRNA, to name only a few instances. Despite that, a significant part of literature reports studies on gene expression with microarray datasets. Single-omics data have high numbers of attributes and very low sample counts. This characteristic makes them paradigmatic of an under-sampled, small-n large-p machine learning problem. An important goal of single-omics data analysis is to find the most relevant genes, in terms of their potential use in clinics and research, in the batch of available data. This problem has been addressed in gene selection as one of the pre-processing steps in data mining. An analysis that use only one type of data (single-omics) often miss the complexity of the landscape of molecular phenomena underlying the disease. As a result, they provide limited and sometimes poorly reliable information about the disease mechanisms. Therefore, in recent years, researchers have been eager to build models that are more complex, obtaining more reliable results using multi-omics data. However, to achieve this, the most important challenge is data integration. In this paper, we provide a comprehensive overview of the challenges in single and multi-omics data analysis of cancer data, focusing on gene selection and data integration methods.}
}

@book{2002molecular,
	author = "Alberts B and Johnson A and Lewis J and others",
	title = "Molecular Biology of the Cell",
	publisher = "4th edition",
	year = 2002
}

@ARTICLE{Coarfa2021-eh,
	title    = "{Reverse-Phase} Protein Array: Technology, Application, Data
	Processing, and Integration",
	author   = "Coarfa, Cristian and Grimm, Sandra L and Rajapakshe, Kimal and
	Perera, Dimuthu and Lu, Hsin-Yi and Wang, Xuan and Christensen,
	Kurt R and Mo, Qianxing and Edwards, Dean P and Huang, Shixia",
	abstract = "Reverse-phase protein array (RPPA) is a high-throughput
	antibody-based targeted proteomics platform that can quantify
	hundreds of proteins in thousands of samples derived from tissue
	or cell lysates, serum, plasma, or other body fluids. Protein
	samples are robotically arrayed as microspots on
	nitrocellulose-coated glass slides. Each slide is probed with a
	specific antibody that can detect levels of total protein
	expression or post-translational modifications, such as
	phosphorylation as a measure of protein activity. Here we
	describe workflow protocols and software tools that we have
	developed and optimized for RPPA in a core facility setting that
	includes sample preparation, microarray mapping and printing of
	protein samples, antibody labeling, slide scanning, image
	analysis, data normalization and quality control, data reporting,
	statistical analysis, and management of data. Our RPPA platform
	currently analyzes ∼240 validated antibodies that primarily
	detect proteins in signaling pathways and cellular processes that
	are important in cancer biology. This is a robust technology that
	has proven to be of value for both validation and discovery
	proteomic research and integration with other omics data sets.",
	journal  = "J Biomol Tech",
	volume   =  32,
	number   =  1,
	pages    = "15--29",
	month    =  apr,
	year     =  2021,
	address  = "United States",
	language = "en"
}

@ARTICLE{Wang2009-wi,
	title    = "{RNA-Seq}: a revolutionary tool for transcriptomics",
	author   = "Wang, Zhong and Gerstein, Mark and Snyder, Michael",
	abstract = "RNA-Seq is a recently developed approach to transcriptome
	profiling that uses deep-sequencing technologies. Studies using
	this method have already altered our view of the extent and
	complexity of eukaryotic transcriptomes. RNA-Seq also provides a
	far more precise measurement of levels of transcripts and their
	isoforms than other methods. This article describes the RNA-Seq
	approach, the challenges associated with its application, and the
	advances made so far in characterizing several eukaryote
	transcriptomes.",
	journal  = "Nat Rev Genet",
	volume   =  10,
	number   =  1,
	pages    = "57--63",
	month    =  jan,
	year     =  2009,
	address  = "England",
	language = "en"
}

@article{DANUSER2011973,
	title = {Computer Vision in Cell Biology},
	journal = {Cell},
	volume = {147},
	number = {5},
	pages = {973-978},
	year = {2011},
	issn = {0092-8674},
	doi = {https://doi.org/10.1016/j.cell.2011.11.001},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867411012906},
	author = {Gaudenz Danuser},
	abstract = {Computer vision refers to the theory and implementation of artificial systems that extract information from images to understand their content. Although computers are widely used by cell biologists for visualization and measurement, interpretation of image content, i.e., the selection of events worth observing and the definition of what they mean in terms of cellular mechanisms, is mostly left to human intuition. This Essay attempts to outline roles computer vision may play and should play in image-based studies of cellular life.}
}

@book{russell2010artificial,
	title={Artificial intelligence a modern approach},
	author={Russell, Stuart J},
	year={2010},
	publisher={Pearson Education, Inc.}
}

@misc{berrar2019cross,
	title={Cross-Validation.},
	author={Berrar, Daniel},
	year={2019}
}

@book{luger2005artificial,
	title={Artificial intelligence: structures and strategies for complex problem solving},
	author={Luger, George F},
	year={2005},
	publisher={Pearson education}
}

@book{10.5555/541177,
	author = {Mitchell, Thomas M.},
	title = {Machine Learning},
	year = {1997},
	isbn = {0070428077},
	publisher = {McGraw-Hill, Inc.},
	address = {USA},
	edition = {1},
	abstract = {This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning--including probability and statistics, artificial intelligence, and neural networks--unifying them all in a logical and coherent manner. Machine Learning serves as a useful reference tool for software developers and researchers, as well as an outstanding text for college students. Table of contents Chapter 1. Introduction Chapter 2. Concept Learning and the General-to-Specific Ordering Chapter 3. Decision Tree Learning Chapter 4. Artificial Neural Networks Chapter 5. Evaluating Hypotheses Chapter 6. Bayesian Learning Chapter 7. Computational Learning Theory Chapter 8. Instance-Based Learning Chapter 9. Inductive Logic Programming Chapter 10. Analytical Learning Chapter 11. Combining Inductive and Analytical Learning Chapter 12. Reinforcement Learning.}
}

@article{hawkins2004problem,
	title={The problem of overfitting},
	author={Hawkins, Douglas M},
	journal={Journal of chemical information and computer sciences},
	volume={44},
	number={1},
	pages={1--12},
	year={2004},
	publisher={ACS Publications}
}

@misc{minmaxscaler,
	howpublished = "\url{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html}"
}

@article{featureSelection,
	author = {Kumar, Vipin},
	year = {2014},
	month = {06},
	pages = {},
	title = {Feature Selection: A literature Review},
	volume = {4},
	journal = {The Smart Computing Review},
	doi = {10.6029/smartcr.2014.03.007}
}

@Inbook{Keogh2017,
	author="Keogh, Eamonn
	and Mueen, Abdullah",
	editor="Sammut, Claude
	and Webb, Geoffrey I.",
	title="Curse of Dimensionality",
	bookTitle="Encyclopedia of Machine Learning and Data Mining",
	year="2017",
	publisher="Springer US",
	address="Boston, MA",
	pages="314--315",
	isbn="978-1-4899-7687-1",
	doi="10.1007/978-1-4899-7687-1_192",
	url="https://doi.org/10.1007/978-1-4899-7687-1_192"
}

@article{goswami2014feature,
	title={Feature selection: A practitioner view},
	author={Goswami, Saptarsi and Chakrabarti, Amlan},
	journal={International Journal of Information Technology and Computer Science (IJITCS)},
	volume={6},
	number={11},
	pages={66},
	year={2014},
	publisher={Citeseer}
}

@article{MOMENI2020103466,
	title = {A survey on single and multi omics data mining methods in cancer data classification},
	journal = {Journal of Biomedical Informatics},
	volume = {107},
	pages = {103466},
	year = {2020},
	issn = {1532-0464},
	doi = {https://doi.org/10.1016/j.jbi.2020.103466},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046420300939},
	author = {Zahra Momeni and Esmail Hassanzadeh and Mohammad {Saniee Abadeh} and Riccardo Bellazzi},
	keywords = {Cancer classification, Single and multi omics data, Gene selection, High dimensional datasets, Data integration},
	abstract = {Data analytics is routinely used to support biomedical research in all areas, with particular focus on the most relevant clinical conditions, such as cancer. Bioinformatics approaches, in particular, have been used to characterize the molecular aspects of diseases. In recent years, numerous studies have been performed on cancer based upon single and multi-omics data. For example, Single-omics-based studies have employed a diverse set of data, such as gene expression, DNA methylation, or miRNA, to name only a few instances. Despite that, a significant part of literature reports studies on gene expression with microarray datasets. Single-omics data have high numbers of attributes and very low sample counts. This characteristic makes them paradigmatic of an under-sampled, small-n large-p machine learning problem. An important goal of single-omics data analysis is to find the most relevant genes, in terms of their potential use in clinics and research, in the batch of available data. This problem has been addressed in gene selection as one of the pre-processing steps in data mining. An analysis that use only one type of data (single-omics) often miss the complexity of the landscape of molecular phenomena underlying the disease. As a result, they provide limited and sometimes poorly reliable information about the disease mechanisms. Therefore, in recent years, researchers have been eager to build models that are more complex, obtaining more reliable results using multi-omics data. However, to achieve this, the most important challenge is data integration. In this paper, we provide a comprehensive overview of the challenges in single and multi-omics data analysis of cancer data, focusing on gene selection and data integration methods.}
}

@Article{Breiman2001,
	author={Breiman, Leo},
	title={Random Forests},
	journal={Machine Learning},
	year={2001},
	month={Oct},
	day={01},
	volume={45},
	number={1},
	pages={5-32},
	abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	issn={1573-0565},
	doi={10.1023/A:1010933404324},
	url={https://doi.org/10.1023/A:1010933404324}
}

@Article{Biau2016,
	author={Biau, G{\'e}rard
	and Scornet, Erwan},
	title={A random forest guided tour},
	journal={TEST},
	year={2016},
	month={Jun},
	day={01},
	volume={25},
	number={2},
	pages={197-227},
	abstract={The random forest algorithm, proposed by L. Breiman in 2001, has been extremely successful as a general-purpose classification and regression method. The approach, which combines several randomized decision trees and aggregates their predictions by averaging, has shown excellent performance in settings where the number of variables is much larger than the number of observations. Moreover, it is versatile enough to be applied to large-scale problems, is easily adapted to various ad hoc learning tasks, and returns measures of variable importance. The present article reviews the most recent theoretical and methodological developments for random forests. Emphasis is placed on the mathematical forces driving the algorithm, with special attention given to the selection of parameters, the resampling mechanism, and variable importance measures. This review is intended to provide non-experts easy access to the main ideas.},
	issn={1863-8260},
	doi={10.1007/s11749-016-0481-7},
	url={https://doi.org/10.1007/s11749-016-0481-7}
}

@Inbook{Ghahramani2004,
	author="Ghahramani, Zoubin",
	editor="Bousquet, Olivier
	and von Luxburg, Ulrike
	and R{\"a}tsch, Gunnar",
	title="Unsupervised Learning",
	bookTitle="Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, T{\"u}bingen, Germany, August 4 - 16, 2003, Revised Lectures",
	year="2004",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="72--112",
	abstract="We give a tutorial and overview of the field of unsupervised learning from the perspective of statistical modeling. Unsupervised learning can be motivated from information theoretic and Bayesian principles. We briefly review basic models in unsupervised learning, including factor analysis, PCA, mixtures of Gaussians, ICA, hidden Markov models, state-space models, and many variants and extensions. We derive the EM algorithm and give an overview of fundamental concepts in graphical models, and inference algorithms on graphs. This is followed by a quick tour of approximate Bayesian inference, including Markov chain Monte Carlo (MCMC), Laplace approximation, BIC, variational approximations, and expectation propagation (EP). The aim of this chapter is to provide a high-level view of the field. Along the way, many state-of-the-art ideas and future directions are also reviewed.",
	isbn="978-3-540-28650-9",
	doi="10.1007/978-3-540-28650-9_5",
	url="https://doi.org/10.1007/978-3-540-28650-9_5"
}


@Inbook{Diday1976,
	author="Diday, E.
	and Simon, J. C.",
	editor="Fu, King Sun",
	title="Clustering Analysis",
	bookTitle="Digital Pattern Recognition",
	year="1976",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="47--94",
	abstract="Cluster analysis is one of the Pattern Recognition techniques and should be appreciated as such. It may be characterized by the use of resemblance or dissemblance measures between the objects to be identified.",
	isbn="978-3-642-96303-2",
	doi="10.1007/978-3-642-96303-2_3",
	url="https://doi.org/10.1007/978-3-642-96303-2_3"
}

@article{morissette2013k,
	title={The k-means clustering technique: General considerations and implementation in Mathematica},
	author={Morissette, Laurence and Chartier, Sylvain},
	journal={Tutorials in Quantitative Methods for Psychology},
	volume={9},
	number={1},
	pages={15--24},
	year={2013}
}
@article{Jimenez1998SupervisedCI,
	title={Supervised classification in high-dimensional space: geometrical, statistical, and asymptotical properties of multivariate data},
	author={Luis O. Jimenez and David A. Landgrebe},
	journal={IEEE Trans. Syst. Man Cybern. Part C},
	year={1998},
	volume={28},
	pages={39-54}
}

@article{kaelbling1996reinforcement,
	title={Reinforcement learning: A survey},
	author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
	journal={Journal of artificial intelligence research},
	volume={4},
	pages={237--285},
	year={1996}
}

@article{butte2002use,
	title={The use and analysis of microarray data},
	author={Butte, Atul},
	journal={Nature reviews drug discovery},
	volume={1},
	number={12},
	pages={951--960},
	year={2002},
	publisher={Nature Publishing Group UK London}
}

@misc{spearman,
	howpublished = "\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html}"
}

@ARTICLE{1453511,
	
	author={Hanchuan Peng and Fuhui Long and Ding, C.},
	
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	
	title={Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy}, 
	
	year={2005},
	
	volume={27},
	
	number={8},
	
	pages={1226-1238},
	
	doi={10.1109/TPAMI.2005.159}
}

@article{JSSv036i11,
	title={Feature Selection with the Boruta Package},
	volume={36},
	url={https://www.jstatsoft.org/index.php/jss/article/view/v036i11},
	doi={10.18637/jss.v036.i11},
	abstract={This article describes a &amp;lt;b&amp;gt;R&amp;lt;/b&amp;gt; package &amp;lt;b&amp;gt;Boruta&amp;lt;/b&amp;gt;, implementing a novel feature selection algorithm for finding emph{all relevant variables}. The algorithm is designed as a wrapper around a Random Forest classification algorithm. It iteratively removes the features which are proved by a statistical test to be less relevant than random probes. The &amp;lt;b&amp;gt;Boruta&amp;lt;/b&amp;gt; package provides a convenient interface to the algorithm. The short description of the algorithm and examples of its application are presented.},
	number={11},
	journal={Journal of Statistical Software},
	author={Kursa, Miron B. and Rudnicki, Witold R.},
	year={2010},
	pages={1–13}
}

@article{Bennett1969TheID,
	title={The intrinsic dimensionality of signal collections},
	author={Robert S. Bennett},
	journal={IEEE Trans. Inf. Theory},
	year={1969},
	volume={15},
	pages={517-525}
}

@ARTICLE{Facco2017-pd,
	title    = "Estimating the intrinsic dimension of datasets by a minimal
	neighborhood information",
	author   = "Facco, Elena and d'Errico, Maria and Rodriguez, Alex and Laio,
	Alessandro",
	abstract = "Analyzing large volumes of high-dimensional data is an issue of
	fundamental importance in data science, molecular simulations and
	beyond. Several approaches work on the assumption that the
	important content of a dataset belongs to a manifold whose
	Intrinsic Dimension (ID) is much lower than the crude large
	number of coordinates. Such manifold is generally twisted and
	curved; in addition points on it will be non-uniformly
	distributed: two factors that make the identification of the ID
	and its exploitation really hard. Here we propose a new ID
	estimator using only the distance of the first and the second
	nearest neighbor of each point in the sample. This extreme
	minimality enables us to reduce the effects of curvature, of
	density variation, and the resulting computational cost. The ID
	estimator is theoretically exact in uniformly distributed
	datasets, and provides consistent measures in general. When used
	in combination with block analysis, it allows discriminating the
	relevant dimensions as a function of the block size. This allows
	estimating the ID even when the data lie on a manifold perturbed
	by a high-dimensional noise, a situation often encountered in
	real world data sets. We demonstrate the usefulness of the
	approach on molecular simulations and image analysis.",
	journal  = "Sci Rep",
	volume   =  7,
	number   =  1,
	pages    = "12140",
	month    =  sep,
	year     =  2017,
	address  = "England",
	language = "en"
}

@article{articleMIC,
	author = {Reshef, David and Reshef, Yakir and Finucane, Hilary and Grossman, Sharon and McVean, Gilean and Turnbaugh, Peter and Lander, Eric and Mitzenmacher, Michael and Sabeti, Pardis},
	year = {2011},
	month = {12},
	pages = {1518-24},
	title = {Detecting Novel Associations in Large Data Sets},
	volume = {334},
	journal = {Science (New York, N.Y.)},
	doi = {10.1126/science.1205438}
}

@ARTICLE{Hu2016-ji,
	title   = "Maximal information-based nonparametric exploration of condition
	monitoring data''",
	author  = "Hu, Y and Palm{\'e}, T and Fink, O",
	journal = "PHM Society European Conference",
	volume  =  3,
	year    =  2016
}

@article{van2009dimensionality,
	title={Dimensionality reduction: a comparative},
	author={Van Der Maaten, Laurens and Postma, Eric and Van den Herik, Jaap and others},
	journal={J Mach Learn Res},
	volume={10},
	number={66-71},
	pages={13},
	year={2009}
}

@article{Jimenez1998SupervisedCI,
	title={Supervised classification in high-dimensional space: geometrical, statistical, and asymptotical properties of multivariate data},
	author={Luis O. Jimenez and David A. Landgrebe},
	journal={IEEE Trans. Syst. Man Cybern. Part C},
	year={1998},
	volume={28},
	pages={39-54}
}

@article{ringner2008principal,
	title={What is principal component analysis?},
	author={Ringn{\'e}r, Markus},
	journal={Nature biotechnology},
	volume={26},
	number={3},
	pages={303--304},
	year={2008},
	publisher={Nature Publishing Group US New York}
}

@article{balakrishnama1998linear,
	title={Linear discriminant analysis-a brief tutorial},
	author={Balakrishnama, Suresh and Ganapathiraju, Aravind},
	journal={Institute for Signal and information Processing},
	volume={18},
	number={1998},
	pages={1--8},
	year={1998},
	publisher={Mississippi}
}

@article{wall2003singular,
	title={Singular value decomposition and principal component analysis},
	author={Wall, Michael E and Rechtsteiner, Andreas and Rocha, Luis M},
	journal={A practical approach to microarray data analysis},
	pages={91--109},
	year={2003},
	publisher={Springer}
}

@misc{https://doi.org/10.48550/arxiv.1802.03426,
	doi = {10.48550/ARXIV.1802.03426},
	
	url = {https://arxiv.org/abs/1802.03426},
	
	author = {McInnes, Leland and Healy, John and Melville, James},
	
	keywords = {Machine Learning (stat.ML), Computational Geometry (cs.CG), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
	
	publisher = {arXiv},
	
	year = {2018},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{van2008visualizing,
	title={Visualizing data using t-SNE.},
	author={Van der Maaten, Laurens and Hinton, Geoffrey},
	journal={Journal of machine learning research},
	volume={9},
	number={11},
	year={2008}
}

@incollection{joyce2011kullback,
	title={Kullback-leibler divergence},
	author={Joyce, James M},
	booktitle={International encyclopedia of statistical science},
	pages={720--722},
	year={2011},
	publisher={Springer}
}

@misc{https://doi.org/10.48550/arxiv.1609.04747,
	doi = {10.48550/ARXIV.1609.04747},
	
	url = {https://arxiv.org/abs/1609.04747},
	
	author = {Ruder, Sebastian},
	
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {An overview of gradient descent optimization algorithms},
	
	publisher = {arXiv},
	
	year = {2016},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{kendall,
	author = {KENDALL, M. G.},
	title = "{A NEW MEASURE OF RANK CORRELATION}",
	journal = {Biometrika},
	volume = {30},
	number = {1-2},
	pages = {81-93},
	year = {1938},
	month = {06},
	issn = {0006-3444},
	doi = {10.1093/biomet/30.1-2.81},
	url = {https://doi.org/10.1093/biomet/30.1-2.81},
	eprint = {https://academic.oup.com/biomet/article-pdf/30/1-2/81/423380/30-1-2-81.pdf},
}

@misc{nelsen2001kendall,
	title={Kendall tau metric. Encyclopaedia of Mathematics},
	author={Nelsen, R},
	year={2001},
	publisher={Springer ISBN}
}

@misc{https://doi.org/10.48550/arxiv.1811.12808,
	doi = {10.48550/ARXIV.1811.12808},
	
	url = {https://arxiv.org/abs/1811.12808},
	
	author = {Raschka, Sebastian},
	
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning},
	
	publisher = {arXiv},
	
	year = {2018},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{gridsearchcv,
	howpublished = "\url{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}"
}

@article{ozata2019piwi,
	title={PIWI-interacting RNAs: small RNAs with big functions},
	author={Ozata, Deniz M and Gainetdinov, Ildar and Zoch, Ansgar and O’Carroll, D{\'o}nal and Zamore, Phillip D},
	journal={Nature Reviews Genetics},
	volume={20},
	number={2},
	pages={89--108},
	year={2019},
	publisher={Nature Publishing Group UK London}
}

@article{cullen2003nuclear,
	title={Nuclear RNA export},
	author={Cullen, Bryan R},
	journal={Journal of cell science},
	volume={116},
	number={4},
	pages={587--597},
	year={2003},
	publisher={Company of Biologists}
}

@article{lee2004short,
	title={A short history of a short RNA},
	author={Lee, Rosalind and Feinbaum, Rhonda and Ambros, Victor and others},
	journal={Cell},
	volume={116},
	number={2 Suppl},
	pages={S89--92},
	year={2004}
}

@article{lu2018microrna,
	title={MicroRNA},
	author={Lu, Thomas X and Rothenberg, Marc E},
	journal={Journal of allergy and clinical immunology},
	volume={141},
	number={4},
	pages={1202--1207},
	year={2018},
	publisher={Elsevier}
}

@misc{microbiota,
	howpublished = "\url{https://www.recentiprogressi.it/archivio/2296/articoli/24680/}"
}

@article{mcknight2010mann,
	title={Mann-Whitney U Test},
	author={McKnight, Patrick E and Najab, Julius},
	journal={The Corsini encyclopedia of psychology},
	pages={1--1},
	year={2010},
	publisher={Wiley Online Library}
}

@article{chai2014root,
	title={Root mean square error (RMSE) or mean absolute error (MAE)?--Arguments against avoiding RMSE in the literature},
	author={Chai, Tianfeng and Draxler, Roland R},
	journal={Geoscientific model development},
	volume={7},
	number={3},
	pages={1247--1250},
	year={2014},
	publisher={Copernicus Publications G{\"o}ttingen, Germany}
}

@article{das2004mean,
	title={Mean squared error of empirical predictor},
	author={Das, Kalyan and Jiang, Jiming and Rao, JNK},
	year={2004}
}

@article{chai2014root,
	title={Root mean square error (RMSE) or mean absolute error (MAE)?--Arguments against avoiding RMSE in the literature},
	author={Chai, Tianfeng and Draxler, Roland R},
	journal={Geoscientific model development},
	volume={7},
	number={3},
	pages={1247--1250},
	year={2014},
	publisher={Copernicus Publications G{\"o}ttingen, Germany}
}

@article{cameron1997r,
	title={An R-squared measure of goodness of fit for some common nonlinear regression models},
	author={Cameron, A Colin and Windmeijer, Frank AG},
	journal={Journal of econometrics},
	volume={77},
	number={2},
	pages={329--342},
	year={1997},
	publisher={Elsevier}
}

@inproceedings{davis2006relationship,
	title={The relationship between Precision-Recall and ROC curves},
	author={Davis, Jesse and Goadrich, Mark},
	booktitle={Proceedings of the 23rd international conference on Machine learning},
	pages={233--240},
	year={2006}
}

@article{10.1371/journal.pone.0118432,
	doi = {10.1371/journal.pone.0118432},
	author = {Saito, Takaya AND Rehmsmeier, Marc},
	journal = {PLOS ONE},
	publisher = {Public Library of Science},
	title = {The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets},
	year = {2015},
	month = {03},
	volume = {10},
	url = {https://doi.org/10.1371/journal.pone.0118432},
	pages = {1-21},
	abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
	number = {3},
	
}

@Inbook{Benesty2009,
	author="Benesty, Jacob
	and Chen, Jingdong
	and Huang, Yiteng
	and Cohen, Israel",
	title="Pearson Correlation Coefficient",
	bookTitle="Noise Reduction in Speech Processing",
	year="2009",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="1--4",
	abstract="This chapter develops several forms of the Pearson correlation coefficient in the different domains. This coefficient can be used as an optimization criterion to derive different optimal noise reduction filters [14], but is even more useful for analyzing these optimal filters for their noise reduction performance.",
	isbn="978-3-642-00296-0",
	doi="10.1007/978-3-642-00296-0_5",
	url="https://doi.org/10.1007/978-3-642-00296-0_5"
}

@Article{Rodon2019,
	author={Rodon, Jordi
	and Soria, Jean-Charles
	and Berger, Raanan
	and Miller, Wilson H.
	and Rubin, Eitan
	and Kugel, Aleksandra
	and Tsimberidou, Apostolia
	and Saintigny, Pierre
	and Ackerstein, Aliza
	and Bra{\~{n}}a, Irene
	and Loriot, Yohann
	and Afshar, Mohammad
	and Miller, Vincent
	and Wunder, Fanny
	and Bresson, Catherine
	and Martini, Jean-Fran{\c{c}}ois
	and Raynaud, Jacques
	and Mendelsohn, John
	and Batist, Gerald
	and Onn, Amir
	and Tabernero, Josep
	and Schilsky, Richard L.
	and Lazar, Vladimir
	and Lee, J. Jack
	and Kurzrock, Razelle},
	title={Genomic and transcriptomic profiling expands precision cancer medicine: the WINTHER trial},
	journal={Nature Medicine},
	year={2019},
	month={May},
	day={01},
	volume={25},
	number={5},
	pages={751-758},
	abstract={Precision medicine focuses on DNA abnormalities, but not all tumors have tractable genomic alterations. The WINTHER trial (NCT01856296) navigated patients to therapy on the basis of fresh biopsy-derived DNA sequencing (arm A; 236 gene panel) or RNA expression (arm B; comparing tumor to normal). The clinical management committee (investigators from five countries) recommended therapies, prioritizing genomic matches; physicians determined the therapy given. Matching scores were calculated post-hoc for each patient, according to drugs received: for DNA, the number of alterations matched divided by the total alteration number; for RNA, expression-matched drug ranks. Overall, 303 patients consented; 107 (35{\%}; 69 in arm A and 38 in arm B) were evaluable for therapy. The median number of previous therapies was three. The most common diagnoses were colon, head and neck, and lung cancers. Among the 107 patients, the rate of stable disease{\thinspace}≥6{\thinspace}months and partial or complete response was 26.2{\%} (arm A: 23.2{\%}; arm B: 31.6{\%} (P{\thinspace}={\thinspace}0.37)). The patient proportion with WINTHER versus previous therapy progression-free survival ratio of >1.5 was 22.4{\%}, which did not meet the pre-specified primary end point. Fewer previous therapies, better performance status and higher matching score correlated with longer progression-free survival (all P{\thinspace}<{\thinspace}0.05, multivariate). Our study shows that genomic and transcriptomic profiling are both useful for improving therapy recommendations and patient outcome, and expands personalized cancer treatment.},
	issn={1546-170X},
	doi={10.1038/s41591-019-0424-4},
	url={https://doi.org/10.1038/s41591-019-0424-4}
}

@article{doi:10.1080/08839514.2019.1637138,
	author = {Anil Jadhav and Dhanya Pramod and Krishnan Ramanathan},
	title = {Comparison of Performance of Data Imputation Methods for Numeric Dataset},
	journal = {Applied Artificial Intelligence},
	volume = {33},
	number = {10},
	pages = {913-933},
	year  = {2019},
	publisher = {Taylor & Francis},
	doi = {10.1080/08839514.2019.1637138},
	
	URL = { 
	
	https://doi.org/10.1080/08839514.2019.1637138
	
	
	
	},
	eprint = { 
	
	https://doi.org/10.1080/08839514.2019.1637138
	
	
	
	}
	
}

@article{lalwani2018implementation,
	title={Implementation of a Chatbot System using AI and NLP},
	author={Lalwani, Tarun and Bhalotia, Shashank and Pal, Ashish and Rathod, Vasundhara and Bisen, Shreya},
	journal={International Journal of Innovative Research in Computer Science \& Technology (IJIRCST) Volume-6, Issue-3},
	year={2018}
}

@article{cambria2014jumping,
	title={Jumping NLP curves: A review of natural language processing research},
	author={Cambria, Erik and White, Bebo},
	journal={IEEE Computational intelligence magazine},
	volume={9},
	number={2},
	pages={48--57},
	year={2014},
	publisher={IEEE}
}

@book{szeliski2022computer,
	title={Computer vision: algorithms and applications},
	author={Szeliski, Richard},
	year={2022},
	publisher={Springer Nature}
}


@article{kingsford2008decision,
	title={What are decision trees?},
	author={Kingsford, Carl and Salzberg, Steven L},
	journal={Nature biotechnology},
	volume={26},
	number={9},
	pages={1011--1013},
	year={2008},
	publisher={Nature Publishing Group US New York}
}

@misc{geneBANK,
	howpublished = "\url{https://en.wikipedia.org/wiki/GenBank}"
}

@misc{covidDeath,
	howpublished = "\url{https://ourworldindata.org/covid-deaths}"
}

@Article{Schubach2017,
	author={Schubach, Max
	and Re, Matteo
	and Robinson, Peter N.
	and Valentini, Giorgio},
	title={Imbalance-Aware Machine Learning for Predicting Rare and Common Disease-Associated Non-Coding Variants},
	journal={Scientific Reports},
	year={2017},
	month={Jun},
	day={07},
	volume={7},
	number={1},
	pages={2959},
	abstract={Disease and trait-associated variants represent a tiny minority of all known genetic variation, and therefore there is necessarily an imbalance between the small set of available disease-associated and the much larger set of non-deleterious genomic variation, especially in non-coding regulatory regions of human genome. Machine Learning (ML) methods for predicting disease-associated non-coding variants are faced with a chicken and egg problem - such variants cannot be easily found without ML, but ML cannot begin to be effective until a sufficient number of instances have been found. Most of state-of-the-art ML-based methods do not adopt specific imbalance-aware learning techniques to deal with imbalanced data that naturally arise in several genome-wide variant scoring problems, thus resulting in a significant reduction of sensitivity and precision. We present a novel method that adopts imbalance-aware learning strategies based on resampling techniques and a hyper-ensemble approach that outperforms state-of-the-art methods in two different contexts: the prediction of non-coding variants associated with Mendelian and with complex diseases. We show that imbalance-aware ML is a key issue for the design of robust and accurate prediction algorithms and we provide a method and an easy-to-use software tool that can be effectively applied to this challenging prediction task.},
	issn={2045-2322},
	doi={10.1038/s41598-017-03011-5},
	url={https://doi.org/10.1038/s41598-017-03011-5}
}

@misc{covidExplorer,
	howpublished = "\url{https://ourworldindata.org/explorers/coronavirus-data-explorer}"
}


@article{papassotiropoulos2006genetics,
	title={Genetics, transcriptomics, and proteomics of Alzheimer's disease},
	author={Papassotiropoulos, Andreas and Fountoulakis, Michael and Dunckley, Travis and Stephan, Dietrich A and Reiman, Eric M},
	journal={Journal of Clinical Psychiatry},
	volume={67},
	number={4},
	pages={652},
	year={2006},
	publisher={Physicians Postgraduate Press; 1999}
}

@article{sager2015transcriptomics,
	title={Transcriptomics in cancer diagnostics: developments in technology, clinical research and commercialization},
	author={Sager, Monica and Yeat, Nai Chien and Pajaro-Van der Stadt, Stefan and Lin, Charlotte and Ren, Qiuyin and Lin, Jimmy},
	journal={Expert Review of Molecular Diagnostics},
	volume={15},
	number={12},
	pages={1589--1603},
	year={2015},
	publisher={Taylor \& Francis}
}

@article{chan2018delineating,
	title={Delineating inflammatory bowel disease through transcriptomic studies: current review of progress and evidence},
	author={Chan, Seow-Neng and Den Low, Eden Ngah and Ali, Raja Affendi Raja and Mokhtar, Norfilza Mohd},
	journal={Intestinal research},
	volume={16},
	number={3},
	pages={374},
	year={2018},
	publisher={Korean Association for the Study of Intestinal Diseases}
}

@article{stenton2020diagnosis,
	title={The diagnosis of inborn errors of metabolism by an integrative “multi-omics” approach: A perspective encompassing genomics, transcriptomics, and proteomics},
	author={Stenton, Sarah L and Kremer, Laura S and Kopajtich, Robert and Ludwig, Christina and Prokisch, Holger},
	journal={Journal of inherited metabolic disease},
	volume={43},
	number={1},
	pages={25--35},
	year={2020},
	publisher={Wiley Online Library}
}

@InProceedings{10.1007/11908029_58,
	author="Rudnicki, Witold R.
	and Kierczak, Marcin
	and Koronacki, Jacek
	and Komorowski, Jan",
	editor="Greco, Salvatore
	and Hata, Yutaka
	and Hirano, Shoji
	and Inuiguchi, Masahiro
	and Miyamoto, Sadaaki
	and Nguyen, Hung Son
	and S{\l}owi{\'{n}}ski, Roman",
	title="A Statistical Method for Determining Importance of Variables in an Information System",
	booktitle="Rough Sets and Current Trends in Computing",
	year="2006",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="557--566",
	abstract="A new method for estimation of attributes' importance for supervised classification, based on the random forest approach, is presented. Essentially, an iterative scheme is applied, with each step consisting of several runs of the random forest program. Each run is performed on a suitably modified data set: values of each attribute found unimportant at earlier steps are randomly permuted between objects. At each step, apparent importance of an attribute is calculated and the attribute is declared unimportant if its importance is not uniformly better than that of the attributes earlier found unimportant. The procedure is repeated until only attributes scoring better than the randomized ones are retained. Statistical significance of the results so obtained is verified. This method has been applied to 12 data sets of biological origin. The method was shown to be more reliable than that based on standard application of a random forest to assess attributes' importance.",
	isbn="978-3-540-49842-1"
}

@misc{manifold,
	howpublished = "\url{https://commons.wikimedia.org/wiki/File:Triangles_(spherical_geometry).jpg}"
}

@misc{tnseDatiNonSeparabili,
	howpublished = "\url{https://distill.pub/2016/misread-tsne/}"
}

@article{abdi2010principal,
	title={Principal component analysis},
	author={Abdi, Herv{\'e} and Williams, Lynne J},
	journal={Wiley interdisciplinary reviews: computational statistics},
	volume={2},
	number={4},
	pages={433--459},
	year={2010},
	publisher={Wiley Online Library}
}

@article{balakrishnama1998linear,
	title={Linear discriminant analysis-a brief tutorial},
	author={Balakrishnama, Suresh and Ganapathiraju, Aravind},
	journal={Institute for Signal and information Processing},
	volume={18},
	number={1998},
	pages={1--8},
	year={1998},
	publisher={Mississippi}
}

@article{abdi2007singular,
	title={Singular value decomposition (SVD) and generalized singular value decomposition},
	author={Abdi, Herv{\'e}},
	journal={Encyclopedia of measurement and statistics},
	volume={907},
	pages={912},
	year={2007},
	publisher={Thousand Oaks (CA) Sage}
}

@article{lisboa2006use,
	title={The use of artificial neural networks in decision support in cancer: a systematic review},
	author={Lisboa, Paulo J and Taktak, Azzam FG},
	journal={Neural networks},
	volume={19},
	number={4},
	pages={408--415},
	year={2006},
	publisher={Elsevier}
}

@article{liew2021investigation,
	title={An investigation of XGBoost-based algorithm for breast cancer classification},
	author={Liew, Xin Yu and Hameed, Nazia and Clos, Jeremie},
	journal={Machine Learning with Applications},
	volume={6},
	pages={100154},
	year={2021},
	publisher={Elsevier}
}

@inproceedings{kabiraj2020breast,
	title={Breast cancer risk prediction using XGBoost and random forest algorithm},
	author={Kabiraj, Sajib and Raihan, M and Alvi, Nasif and Afrin, Marina and Akter, Laboni and Sohagi, Shawmi Akhter and Podder, Etu},
	booktitle={2020 11th international conference on computing, communication and networking technologies (ICCCNT)},
	pages={1--4},
	year={2020},
	organization={IEEE}
}

@article{zhang2020novel,
	title={A novel XGBoost method to identify cancer tissue-of-origin based on copy number variations},
	author={Zhang, Yulin and Feng, Tong and Wang, Shudong and Dong, Ruyi and Yang, Jialiang and Su, Jionglong and Wang, Bo},
	journal={Frontiers in genetics},
	volume={11},
	pages={585029},
	year={2020},
	publisher={Frontiers Media SA}
}

@misc{i5-13600k,
	howpublished = "\url{https://www.intel.it/content/www/it/it/products/sku/230493/intel-core-i513600k-processor-24m-cache-up-to-5-10-ghz/specifications.html}"
}

@article{chawla2002smote,
	title={SMOTE: synthetic minority over-sampling technique},
	author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
	journal={Journal of artificial intelligence research},
	volume={16},
	pages={321--357},
	year={2002}
}

@misc{MAtrixRow,
	howpublished = "\url{https://www.prometheus-studio.it/prometheus_blog_wp/2019/09/29/cosa-e-una-matrice-di-confusione/}"
}

@inproceedings{irvin2019chexpert,
	title={Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison},
	author={Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and others},
	booktitle={Proceedings of the AAAI conference on artificial intelligence},
	volume={33},
	number={01},
	pages={590--597},
	year={2019}
}
@article{ma2020survey,
	title={Survey on deep learning for pulmonary medical imaging},
	author={Ma, Jiechao and Song, Yang and Tian, Xi and Hua, Yiting and Zhang, Rongguo and Wu, Jianlin},
	journal={Frontiers of medicine},
	volume={14},
	pages={450--469},
	year={2020},
	publisher={Springer}
}

@article{bressem2020comparing,
	title={Comparing different deep learning architectures for classification of chest radiographs},
	author={Bressem, Keno K and Adams, Lisa C and Erxleben, Christoph and Hamm, Bernd and Niehues, Stefan M and Vahldiek, Janis L},
	journal={Scientific reports},
	volume={10},
	number={1},
	pages={13590},
	year={2020},
	publisher={Nature Publishing Group UK London}
}

@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770--778},
	year={2016}
}

@article{narin2021automatic,
	title={Automatic detection of coronavirus disease (covid-19) using x-ray images and deep convolutional neural networks},
	author={Narin, Ali and Kaya, Ceren and Pamuk, Ziynet},
	journal={Pattern Analysis and Applications},
	volume={24},
	pages={1207--1220},
	year={2021},
	publisher={Springer}
}

@inproceedings{szegedy2017inception,
	title={Inception-v4, inception-resnet and the impact of residual connections on learning},
	author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
	booktitle={Proceedings of the AAAI conference on artificial intelligence},
	volume={31},
	number={1},
	year={2017}
}

@article{das2020truncated,
	title={Truncated inception net: COVID-19 outbreak screening using chest X-rays},
	author={Das, Dipayan and Santosh, KC and Pal, Umapada},
	journal={Physical and engineering sciences in medicine},
	volume={43},
	pages={915--925},
	year={2020},
	publisher={Springer}
}

@article{apostolopoulos2020covid,
	title={Covid-19: automatic detection from x-ray images utilizing transfer learning with convolutional neural networks},
	author={Apostolopoulos, Ioannis D and Mpesiana, Tzani A},
	journal={Physical and engineering sciences in medicine},
	volume={43},
	pages={635--640},
	year={2020},
	publisher={Springer}
}

@article{shibly2020covid,
	title={COVID faster R--CNN: A novel framework to Diagnose Novel Coronavirus Disease (COVID-19) in X-Ray images},
	author={Shibly, Kabid Hassan and Dey, Samrat Kumar and Islam, Md Tahzib-Ul and Rahman, Md Mahbubur},
	journal={Informatics in Medicine Unlocked},
	volume={20},
	pages={100405},
	year={2020},
	publisher={Elsevier}
}

@inproceedings{gaffert2016towards,
	title={Towards an MI-proper predictive mean matching},
	author={Gaffert, Philipp and Meinfelder, Florian and Bosch, Volker},
	booktitle={Conf Proc},
	year={2016}
}
@article{waljee2013comparison,
	title={Comparison of imputation methods for missing laboratory data in medicine},
	author={Waljee, Akbar K and Mukherjee, Ashin and Singal, Amit G and Zhang, Yiwei and Warren, Jeffrey and Balis, Ulysses and Marrero, Jorge and Zhu, Ji and Higgins, Peter DR},
	journal={BMJ open},
	volume={3},
	number={8},
	pages={e002847},
	year={2013},
	publisher={British Medical Journal Publishing Group}
}

@misc{t-studentFunction,
	howpublished = "\url{https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html}"
}

